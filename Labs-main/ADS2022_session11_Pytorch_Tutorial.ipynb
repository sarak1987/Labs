{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cde9b7e",
   "metadata": {},
   "source": [
    "$$\\text{Applied Data Science (CUSP-GX 6001)} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ab114",
   "metadata": {},
   "source": [
    "## Introduction to Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd8b02c",
   "metadata": {},
   "source": [
    "PyTorch is an open source machine learning library used for developing and training neural network based deep learning models. <br>\n",
    "It is developed by Facebook’s AI research group and can be used with Python as well as C++. PyTorch leverages the popularity and flexibility of Python while keeping the convenience and functionality of the original Torch library.  ( Torch is a scientific computing framework developed to efficiently handle complex linear algebra tasks on GPU that would otherwise require a lot of processing time on CPUs) .\n",
    "\n",
    "\n",
    "Unlike TensorFlow (from Google), which use static computation graphs, PyTorch uses dynamic computation, which allows greater flexibility in building complex architectures (although TensorFlow 2.0 has started to incorporate dynamic computations) <br>\n",
    "Similary Keras is a another deep learning API written in Python developed by Google, running on top of the machine learning platform TensorFlow. It was developed with a focus on enabling fast experimentation. Starting from Tensorflow 2.0, Keras has also been incorporated into Tensorflow 2.0\n",
    "\n",
    "The major difference between these 2 libraries is that Pytorch delivers a more flexible environment with the price of slightly reduced automation. This environment is a better pick for a team that has a deeper understanding of deep learning concepts and therefore it is more popularly used in research settings. <br>\n",
    "When it comes to TensorFlow in daily work, the framework delivers a more concise, simpler API. This makes the project less bloated and the code more elegant. A simple training loop requires five lines of code in PyTorch and only one in TensorFlow.\n",
    "This makes Tensorflow more popular in deployment and it has alot of support in production as well compared to Pytorch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe769f",
   "metadata": {},
   "source": [
    "$$\\text{Complete ML Workflow in Pytorch tutorial}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69981730",
   "metadata": {},
   "source": [
    "## 1. Working with Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d3cdb",
   "metadata": {},
   "source": [
    "PyTorch has two primitives to work with data: <b>torch.utils.data.DataLoader</b> and <b>torch.utils.data.Dataset</b>. <br>\n",
    "Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset and gives us data in batches <br>\n",
    "You can use default pre-loaded datasets from pytorch libraries like torchvision or you could use your own custom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1713a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f5ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation to the dataset including normalisation and converting data into a pytorch tensor\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd3dbb6",
   "metadata": {},
   "source": [
    "Pre loaded MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f68dfe",
   "metadata": {},
   "source": [
    "MNIST is the de facto “hello world” dataset of deep learning problems for computer vision. Its a classic dataset of handwritten digits images (0 - 9) used for training various deep learning models. It contains 60,000 training images and 10,000 testing images and each image is 28x28 pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e32f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\n",
    "\n",
    "test_dataset = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7a0315",
   "metadata": {},
   "source": [
    "Custom defined dataset. We only need to implement 2 functions getitem and len which returns i'th data and length of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58376b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self, args):\n",
    "        # initialise\n",
    "        pass\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # code to get ith data element in the dataset (i = index)\n",
    "        return (data, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        # size of your dataset\n",
    "        return count "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a283ff",
   "metadata": {},
   "source": [
    "## 2. Creating a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a6755e",
   "metadata": {},
   "source": [
    "We define our neural network by subclassing nn.Module, and initialize the neural network layers in <b>\\_\\_init__</b>. <br>\n",
    "Every nn.Module subclass implements the operations on input data in the <b>forward</b> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68ef931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33b44604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # The input layer is 28*28 image which is flattened to 784\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        #Define the hidden layers and output layer.\n",
    "        self.hidden1 = nn.Linear(784, 128)\n",
    "        self.hidden2 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 10)\n",
    "        \n",
    "        # Define relu activation and softmax output \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.flatten(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe9506c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (hidden1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (hidden2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(Network())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c6b41",
   "metadata": {},
   "source": [
    "This model is a simple neural network with 1 input layer, 1 output layer and 2 hidden layer\n",
    "The model looks like as follows:\n",
    "\n",
    "<img src=\"1.jpeg\" width=\"600\">\n",
    "\n",
    "This type of model is used for creating non linear classifiers which are better at predicting a class of a given dataset when there is a non linear relationship between labels of class and input features where the inputs are numeric features describing the data. <br>\n",
    "However we will use image dataset here for the purpose of the tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8e046",
   "metadata": {},
   "source": [
    "Move the model to GPU for faster computation <br>\n",
    "In case GPU is unavailable, the code remains the same, however all the computations will be performed on the cpu which might be slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "018e6b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Network().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b5ce74",
   "metadata": {},
   "source": [
    "## 3. Calculating Gradients of loss with Backpropogation using Pytorch's Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f684a6c",
   "metadata": {},
   "source": [
    "When training neural networks, the most frequently used algorithm is <b>back propagation</b>. In this algorithm, parameters (model weights) are adjusted according to the gradient of the loss function with respect to the given parameter. <br>\n",
    "To compute those gradients, PyTorch has a built-in differentiation engine called <b>torch.autograd</b>. It supports automatic computation of gradient for any computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a6b18e",
   "metadata": {},
   "source": [
    "Consider a simple example where x is the input, w and b are learnable parameters (which needs to be optimized) and y is true label <br>\n",
    "$$ \\text{z = x*w + b} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef73dff",
   "metadata": {},
   "source": [
    "![Gradients](2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fb1a155",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss_fn = torch.nn.functional.binary_cross_entropy_with_logits\n",
    "loss = loss_fn(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd436ca1",
   "metadata": {},
   "source": [
    "The most popular loss function used for classification problems is Cross Entropy Loss. <br>\n",
    "In the above example we use binary cross entropy loss because we are predicting just 2 classes. <br>\n",
    "And for our mnist dataset, we will use cross entropy with 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a9c9d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x000002878B5BB5E0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x000002878B5BB7F0>\n"
     ]
    }
   ],
   "source": [
    "# Check the gradient function\n",
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7e126e",
   "metadata": {},
   "source": [
    "Pytorch automatically remembers all the operation (eg add / multiply / relu) performed on the learnable parameters which have requires_grad=True (w and b) <br>\n",
    "Since these operations are remembered/accumulated for each tensor when they are applied, we need to remove them after the end of each epoch using zero_grad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb68f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an optimizer for training\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "# Remove the previous accumulated gradient\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666e24d",
   "metadata": {},
   "source": [
    "To optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function with respect to parameters, namely, we need $$\\frac{\\partial loss}{\\partial w}$$ and $$\\frac{\\partial loss}{\\partial b}$$ under some fixed values of x and y. Pytorch automatically calcualtes these gradients using autograd with backward function in one line loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a77a5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3140, 0.1237, 0.1319],\n",
      "        [0.3140, 0.1237, 0.1319],\n",
      "        [0.3140, 0.1237, 0.1319],\n",
      "        [0.3140, 0.1237, 0.1319],\n",
      "        [0.3140, 0.1237, 0.1319]])\n",
      "tensor([0.3140, 0.1237, 0.1319])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d14467",
   "metadata": {},
   "source": [
    "Finally, in order to update the weights using <br>\n",
    "$$ w_{t+1} = w_t - lr*\\frac{\\partial loss}{\\partial w} $$ and $$ b_{t+1} = b_t - lr*\\frac{\\partial loss}{\\partial b} $$\n",
    "we do optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f13beeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a2322b",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c843c8",
   "metadata": {},
   "source": [
    "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model’s parameters using user defined optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "941da5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train() # Training mode \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device) # send input to GPU\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66427a76",
   "metadata": {},
   "source": [
    "We also check the out of sample validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e6603d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() # Evaluation mode \n",
    "    test_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad(): # no gradient accumulation for validation set\n",
    "        \n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device) # send input to GPU\n",
    "            \n",
    "            # get prediction\n",
    "            pred = model(X) \n",
    "            \n",
    "            # get accuracy\n",
    "            _, predicted = torch.max(pred.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            \n",
    "            # calculate loss\n",
    "            test_loss += loss_fn(pred, y).item() \n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} Accuracy: {100 * correct // total:>8f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2feb3a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.303717  [    0/60000]\n",
      "loss: 2.303586  [ 6400/60000]\n",
      "loss: 2.299951  [12800/60000]\n",
      "loss: 2.300389  [19200/60000]\n",
      "loss: 2.302530  [25600/60000]\n",
      "loss: 2.302290  [32000/60000]\n",
      "loss: 2.302339  [38400/60000]\n",
      "loss: 2.301160  [44800/60000]\n",
      "loss: 2.301150  [51200/60000]\n",
      "loss: 2.300322  [57600/60000]\n",
      "Test Error: \n",
      " Avg loss: 2.300294 Accuracy: 11.000000\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.301506  [    0/60000]\n",
      "loss: 2.301316  [ 6400/60000]\n",
      "loss: 2.298073  [12800/60000]\n",
      "loss: 2.297502  [19200/60000]\n",
      "loss: 2.300217  [25600/60000]\n",
      "loss: 2.300142  [32000/60000]\n",
      "loss: 2.300110  [38400/60000]\n",
      "loss: 2.299390  [44800/60000]\n",
      "loss: 2.298552  [51200/60000]\n",
      "loss: 2.297819  [57600/60000]\n",
      "Test Error: \n",
      " Avg loss: 2.297834 Accuracy: 13.000000\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.299011  [    0/60000]\n",
      "loss: 2.298731  [ 6400/60000]\n",
      "loss: 2.296022  [12800/60000]\n",
      "loss: 2.294166  [19200/60000]\n",
      "loss: 2.297592  [25600/60000]\n",
      "loss: 2.297807  [32000/60000]\n",
      "loss: 2.297604  [38400/60000]\n",
      "loss: 2.297398  [44800/60000]\n",
      "loss: 2.295593  [51200/60000]\n",
      "loss: 2.294893  [57600/60000]\n",
      "Test Error: \n",
      " Avg loss: 2.294947 Accuracy: 16.000000\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.296107  [    0/60000]\n",
      "loss: 2.295684  [ 6400/60000]\n",
      "loss: 2.293568  [12800/60000]\n",
      "loss: 2.290056  [19200/60000]\n",
      "loss: 2.294348  [25600/60000]\n",
      "loss: 2.295135  [32000/60000]\n",
      "loss: 2.294738  [38400/60000]\n",
      "loss: 2.294965  [44800/60000]\n",
      "loss: 2.292043  [51200/60000]\n",
      "loss: 2.291294  [57600/60000]\n",
      "Test Error: \n",
      " Avg loss: 2.291404 Accuracy: 19.000000\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.292644  [    0/60000]\n",
      "loss: 2.292027  [ 6400/60000]\n",
      "loss: 2.290490  [12800/60000]\n",
      "loss: 2.284857  [19200/60000]\n",
      "loss: 2.290276  [25600/60000]\n",
      "loss: 2.291970  [32000/60000]\n",
      "loss: 2.291311  [38400/60000]\n",
      "loss: 2.291787  [44800/60000]\n",
      "loss: 2.287440  [51200/60000]\n",
      "loss: 2.286625  [57600/60000]\n",
      "Test Error: \n",
      " Avg loss: 2.286873 Accuracy: 21.000000\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85793ded",
   "metadata": {},
   "source": [
    "In just 5 epochs, we can see the Cross Entropy loss for the entire training data 60,000 images goes down and increase in accuracy for the validation set from 11% to 21% which means the neural network is learning something. As we increase the number of epochs, number of hidden layers we can see much more improvement. We used a basic shallow neural network which are not known to be good for classifying images. For that purpose, we use Convolutional Neural Networks but the aim for this tutorial is only to look at how to implement basic Pytorch models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8dd592",
   "metadata": {},
   "source": [
    "### References :\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/intro.html <br>\n",
    "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html <br>\n",
    "https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html <br>\n",
    "https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4853c6de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
